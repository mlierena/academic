# Web crawling with elixir

* 수집하는 데이터 : 쇼핑몰의 카테고리, 가격, 이미지 ...
  * 큰 카테고리 -> 서브 카테고리 -> 상품 -> 상품에 대한 정보(크롤링하고자 하는 정보)
  * 250개의 사이트를 동시에 크롤링 해야함

* 각 사이트별로 HTML, JSON 정보 수집.
  * 각 사이트 별로 데이터 정보도 각기 다른 파서가 필요함
  * Product 라는 내부 데이터베이스로 저장해야함. 저장하기 전에 Save Queue를 거쳐서 저장.
    * 병목 현상

* Crawl speed > Site speed : Block
* Crawl speed > DB speed : Out of Memory
  * Demand-driven -> GenStage

* Memory usage
  * Producer driven : Continuously increasing
  * Demand-driven : Fixed

* Site rate limit : `TokenBucket`
  * 허용되는 속도보다 빠르게 들어오면 전부 소진...?
  * 토큰은 시간당 n개만 생성...
    * 1분에 60개만 요청을 보낼 수 있다.
  * Elixir에도 관련 라이브러리가 있다.

* Network Requests Overflow
  * 각 HTTP Request에 Priority Queue를 도입
    * 페이지 디테일에 우선순위를 높인다

* GenServer vs Task
  * Task는 서비스를 제공하지 않는다.
    * handle_call 이 없음
    * 다른 서비스를 사용할 수 있다.
* Task.Supervisor.async
  * exit 트랩이 없음
  * Caller 프로세스도 죽음
  * task를 되살리지도 않음
  * Task를 Supervisor Tree에 넣기 위한 목적
  * Task.async와 비슷함
* Task.async
  * Supervisor와의 관계를 생성하지 않음
